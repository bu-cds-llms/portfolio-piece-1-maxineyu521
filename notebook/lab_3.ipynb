{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13cb692",
   "metadata": {},
   "source": [
    "# Implementation and Analysis of Scaled Dot-Product Attention\n",
    "\n",
    "## 1. Project Objective\n",
    "The goal of this notebook is to build the **Scaled Dot-Product Attention** mechanism from the ground up using PyTorch. Rather than treating the Transformer as a \"black box,\" we will implement and critically analyze the internal components that make modern NLP possible. \n",
    "\n",
    "Specifically, this project investigates:\n",
    "* **The Scaling Factor:** How the mathematical trick of dividing by $\\sqrt{d_k}$ prevents gradient instability in large models.\n",
    "* **The Masking Logic:** How we programmatically \"hide\" information (such as padding or future tokens) to ensure valid computation.\n",
    "* **Semantic Alignment:** How the model utilizes self-attention to \"link\" related words and resolve context (e.g., distinguishing \"bank\" as a river edge vs. a financial institution).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Core Mechanism: Mathematical Foundation\n",
    "\n",
    "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Interpreting the Variables\n",
    "To understand the \"physical\" meaning of these variables, we can view them as a **content-based retrieval system**:\n",
    "\n",
    "* **$Q$ (Query):** Represents the \"Question.\" It is a vector that describes what the current word is looking for in the rest of the sentence.\n",
    "* **$K$ (Key):** Represents the \"Label.\" Every word in the sequence has a key that describes its own characteristics, allowing the Query to determine how relevant this word is.\n",
    "* **$V$ (Value):** Represents the \"Content.\" This is the actual information we want to extract once we have determined which keys match our query.\n",
    "* **$d_k$:** The dimensionality of the keys. We scale by **$\\sqrt{d_k}$** to normalize the variance of our scores. Without this, the dot-product values could grow extremely large, causing the Softmax function to \"saturate\" and leading to the **vanishing gradient problem**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot-product attention.\n",
    "    Formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "    \n",
    "    Args:\n",
    "        query: Tensor of shape (batch_size, seq_len_q, d_k)\n",
    "        key:   Tensor of shape (batch_size, seq_len_k, d_k)\n",
    "        value: Tensor of shape (batch_size, seq_len_v, d_v)\n",
    "        mask:  Optional mask tensor (e.g., for padding or look-ahead)\n",
    "    \n",
    "    Returns:\n",
    "        output: Weighted sum of values (context vectors)\n",
    "        attention_weights: The normalized attention scores (probability distribution)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get the dimension size (d_k) from the query's last dimension\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # 2. Compute Dot Product (Similarity Scores)\n",
    "    # if Q and K more similar, the score will be higher. \n",
    "    # Shape: (batch_size, seq_len_q, seq_len_k)\n",
    "    # the result is Sequence_length_q x Sequence_length_k, which represents how much each query attends to each key.\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    \n",
    "    # 3. Scale the scores\n",
    "    # This prevents gradients from vanishing when dimensions are large\n",
    "    # Softmax sensitivity to large values can cause issues\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # 4. Apply Mask \n",
    "    # If a mask is provided, fill masked positions with a very large negative number\n",
    "    # this way more practical than zero, because softmax will assign near-zero probabilities to these positions.\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # 5. Apply Softmax\n",
    "    # Converts scores into probabilities (summing to 1 across the last dimension)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 6. Weighted Sum (Matrix Multiplication with Value)\n",
    "    # Shape: (batch_size, seq_len_q, d_v)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10eef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, title=\"Attention Weights\"):\n",
    "\n",
    "    # check if attention_weights has a batch dimension and select the first example for visualization\n",
    "    # these work independently of whether the input is from a single example or a batch\n",
    "    if attention_weights.dim() == 3:\n",
    "        weights = attention_weights[0].detach().numpy()\n",
    "    else:\n",
    "        weights = attention_weights.detach().numpy()\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights, annot=True, cmap='viridis', fmt=\".2f\", square=True)\n",
    "    plt.xlabel(\"Keys (Input Sequence Positions)\")\n",
    "    plt.ylabel(\"Queries (Output Sequence Positions)\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"../output/{title.replace(' ', '_').lower()}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db1b66",
   "metadata": {},
   "source": [
    "## Experiment 1: Basic Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dab216",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 4\n",
    "d_k = 8  # Dimension of keys/queries\n",
    "d_v = 8  # Dimension of values\n",
    "\n",
    "# Create random tensors for Q, K, V\n",
    "# Query = \"searching for\", Key = \"content tags\", Value = \"actual content\"\n",
    "query = torch.randn(batch_size, seq_len, d_k)\n",
    "key = torch.randn(batch_size, seq_len, d_k)\n",
    "value = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention Weights shape: {weights.shape}\")\n",
    "visualize_attention(weights, title=\"Random Cross-Attention Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d79fe1",
   "metadata": {},
   "source": [
    "### Critical Analysis:\n",
    "**1. The Yellow Square at (2, 2)**\n",
    "\n",
    "* **What it means:** Row 2 (Query 2) is paying a massive amount of attention to Column 2 (Key 2). Since it's the brightest color, the attention weight is more than 60%.\n",
    "To understand what I mean in this sentence, I don't need help from anyone else. I just need to look at my own definition.\"\n",
    "\n",
    "**2. The Green Squares at (3, 4) and (4, 4)**\n",
    "* **What it means:** Row 3 (Query 3) is looking at Column 4 (Key 4), and Row 4 (Query 4) is also looking at Column 4 (Key 4). Green usually means a moderate-to-high percentage (like 40% to 60%).\n",
    "* **The translation:** The 4th word contains some really important clues! Both the 3rd word and the 4th word need information from the **4th word** to make sense.\n",
    "\n",
    "**3. Why isn't it a perfect diagonal?**\n",
    "\n",
    "If the heatmap were a perfect diagonal line, the model would just be reading a dictionary—each word standing alone. Because the diagonal is broken, it proves the model is acting like a human: **it is looking ahead and pulling context from other parts of the sentence to understand the bigger picture.**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65587753",
   "metadata": {},
   "source": [
    "## Experiment 2: Self-Attention (Q = K = V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18074c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embeddings = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],  # Token 0\n",
    "    [0.0, 1.0, 0.0, 1.0],  # Token 1 (Different) because every element is different from token 0\n",
    "    [1.0, 0.0, 1.0, 0.0]   # Token 2 (Same as Token 0)\n",
    "]).unsqueeze(0)\n",
    "\n",
    "q = sequence_embeddings\n",
    "k = sequence_embeddings\n",
    "v = sequence_embeddings\n",
    "\n",
    "output_self, weights_self = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(\"Visualizing Self-Attention (Note: Token 0 and 2 are identical)\")\n",
    "visualize_attention(weights_self, title=\"Exp 2: Self-Attention (Look for patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2189c0c",
   "metadata": {},
   "source": [
    "### Critical Analysis:\n",
    "**1. The \"Different\" Token is Isolated (Row 1 / Col 1)**\n",
    "* Look at **Row 1**. Token 1 pays nearly **100% of its attention to itself** (the bright square at `(1, 1)`). \n",
    "* Because its features `[0, 1, 0, 1]` do not overlap at all with Token 0 or Token 2, the dot-product similarity is exactly 0. As a result, the squares at `(1, 0)` and `(1, 2)` are dark. The model successfully ignores irrelevant information.\n",
    "\n",
    "**2. The \"Same\" Tokens Share Attention (Rows 0 & 2)**\n",
    "* Look at **Row 0** and **Row 2**. Because Token 0 and Token 2 are perfectly identical, they are equally relevant to each other.\n",
    "* Instead of only looking at themselves (a perfect diagonal), Token 0 **splits its attention evenly** between itself `(0, 0)` and Token 2 `(0, 2)`. \n",
    "* The same happens for Token 2, creating bright squares at both `(2, 0)` and `(2, 2)`. \n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "This single heatmap perfectly demonstrates how self-attention works at a fundamental level. It proves that the attention mechanism acts as a dynamic router: it isolates dissimilar context (ignoring Token 1) while strongly linking similar or repeated context (connecting Token 0 and Token 2) regardless of their distance in the sequence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2479c3",
   "metadata": {},
   "source": [
    "### Resolving Ambiguity through Semantic Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# definition: [Financial, Nature/Water, Building, Shore/Surface]\n",
    "# Token 0: \"River\" -> high Nature, medium Shore\n",
    "river = torch.tensor([0.0, 1.0, 0.0, 0.5])\n",
    "# Token 1: \"Water\" -> very high Nature\n",
    "water = torch.tensor([0.0, 1.2, 0.0, 0.0])\n",
    "# Token 2: \"Bank (Finance)\" -> high Financial, high Building (distractor)\n",
    "bank_finance = torch.tensor([1.2, 0.0, 1.0, 0.0])\n",
    "# Token 3: \"Bank (Shore)\" -> high Nature, high Shore (target)\n",
    "bank_shore = torch.tensor([0.1, 0.9, 0.0, 1.0])\n",
    "\n",
    "\n",
    "# when Query is \"Bank (Shore)\", which token will it attend to?\n",
    "semantic_embeddings = torch.stack([river, water, bank_finance, bank_shore]).unsqueeze(0)\n",
    "\n",
    "# Compute self-attention\n",
    "q = k = v = semantic_embeddings\n",
    "d_k = q.size(-1)\n",
    "scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "labels = [\"River\", \"Water\", \"Bank(Finance)\", \"Bank(Shore)\"]\n",
    "sns.heatmap(weights[0].detach().numpy(), annot=True, fmt=\".2f\", \n",
    "            xticklabels=labels, yticklabels=labels, cmap='YlGnBu')\n",
    "plt.title(\"Semantic Linking & Disambiguation\")\n",
    "plt.xlabel(\"Keys (Contextual Evidence)\")\n",
    "plt.ylabel(\"Queries (Target Words)\")\n",
    "plt.savefig(\"../output/semantic_linking_disambiguation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bc61f",
   "metadata": {},
   "source": [
    "### Critical Analysis:\n",
    "\n",
    "I defined four tokens: *River*, *Water*, *Bank (Finance)*, and *Bank (Shore)* across four semantic dimensions: `[Financial, Nature, Building, Shore]`.\n",
    "\n",
    "1. **Contextual Alignment:** As shown in the heatmap, when the Query is **\"Bank (Shore)\"** (Row 3), it assigns high attention weights to **\"River\"** (0.28+) and **\"Water\"** (0.24+) while virtually ignoring **\"Bank (Finance)\"** (Row 3, Col 2). \n",
    "2. **Mathematical Explanation:** This occurs because the dot product measures the overlap between feature dimensions. Since \"Bank (Shore)\" and \"River\" both share high values in the **Nature** and **Shore** dimensions, their product is large. Conversely, \"Bank (Finance)\" resides in the **Financial/Building** subspace, resulting in a near-zero dot product with natural terms.\n",
    "\n",
    "* **Why this proves \"Intelligence\":**\n",
    "The model uses these scores to \"pull\" the meaning of **River** into the representation of **Bank**. By the time the data reaches the next layer, the vector for \"Bank\" no longer just represents a generic word; it has been \"colored\" by the presence of \"Water,\" effectively resolving the lexical ambiguity. \n",
    "\n",
    "**Limitation:** This experiment uses hard-coded features. In a production Transformer (like BERT), these features are not manually assigned but are learned patterns (Latent Dimensions) discovered through pre-training on billions of tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256394a",
   "metadata": {},
   "source": [
    "## Experiment 3: Impact of Scaling (d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_d_k = 512\n",
    "q_large = torch.randn(1, seq_len, large_d_k)\n",
    "k_large = torch.randn(1, seq_len, large_d_k)\n",
    "v_large = torch.randn(1, seq_len, large_d_k)\n",
    "\n",
    "# Without scaling\n",
    "scores_raw = torch.matmul(q_large, k_large.transpose(-2, -1))\n",
    "print(f\"Variance of scores WITHOUT scaling: {torch.var(scores_raw).item():.2f}\")\n",
    "# Weights spike, gradients vanish\n",
    "weights_no_scale = F.softmax(scores_raw, dim=-1) \n",
    "\n",
    "# With scaling\n",
    "scores_scaled = scores_raw / math.sqrt(large_d_k)\n",
    "print(f\"Variance of scores WITH scaling: {torch.var(scores_scaled).item():.2f}\")\n",
    "# Weights are more distributed, better for training\n",
    "weights_scaled = F.softmax(scores_scaled, dim=-1)\n",
    "\n",
    "print(\"\\nComparing the first row of softmax weights:\")\n",
    "print(\"No Scale (Peaky):\", weights_no_scale[0][0].tolist())\n",
    "print(\"Scaled (Smoother):\", weights_scaled[0][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58838eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup: Large d_k more realistic to see the effect of scaling\n",
    "d_k = 512\n",
    "seq_len = 10 \n",
    "torch.manual_seed(42)\n",
    "q = torch.randn(1, seq_len, d_k)\n",
    "k = torch.randn(1, seq_len, d_k)\n",
    "\n",
    "# 2. Compute Raw Scores (QK^T)\n",
    "# The variance of scores_raw will be approximately d_k (512)\n",
    "scores_raw = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "# 3. Compute Scaled Scores (QK^T / sqrt(d_k))\n",
    "# The variance of scores_scaled will be pushed back to approximately 1\n",
    "scores_scaled = scores_raw / math.sqrt(d_k)\n",
    "\n",
    "# 4. Apply Softmax to both\n",
    "weights_no_scale = F.softmax(scores_raw, dim=-1)\n",
    "weights_scaled = F.softmax(scores_scaled, dim=-1)\n",
    "\n",
    "# 5. Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot A: Without Scaling\n",
    "sns.heatmap(weights_no_scale[0].detach().numpy(), annot=False, cmap='viridis', ax=ax1)\n",
    "ax1.set_title(f\"Figure A: No Scaling (d_k={d_k})\\nVariance ≈ {torch.var(scores_raw).item():.1f}\")\n",
    "ax1.set_xlabel(\"Keys\")\n",
    "ax1.set_ylabel(\"Queries\")\n",
    "\n",
    "# Plot B: With Scaling\n",
    "sns.heatmap(weights_scaled[0].detach().numpy(), annot=False, cmap='viridis', ax=ax2)\n",
    "ax2.set_title(f\"Figure B: With Scaling (1/sqrt(d_k))\\nVariance ≈ {torch.var(scores_scaled).item():.1f}\")\n",
    "ax2.set_xlabel(\"Keys\")\n",
    "ax2.set_ylabel(\"Queries\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/scaling_effect_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd189476",
   "metadata": {},
   "source": [
    "### Critical Analysis: Why Scaling Matters\n",
    "\n",
    "#### 1. The \"Peaky\" Distribution Problem (Figure A)\n",
    "without the scaling factor ($1/\\sqrt{d_k}$), the attention heatmap becomes **extremely sparse**. Most cells are nearly purple (0), while only one or two cells per row are bright yellow (1). \n",
    "\n",
    "**Why does this happen?**\n",
    "When the dimensionality $d_k$ is large, the dot product of $Q$ and $K$ results in values with very high variance (approximately equal to $d_k$). When these large values are passed into the **Softmax** function ($e^{x_i} / \\sum e^{x_j}$), the exponential nature of the function causes the largest score to dominate the entire distribution. \n",
    "**the model only attends to a single token, ignoring all other context.**\n",
    "\n",
    "#### 2. The Vanishing Gradient Problem\n",
    "The derivative of the Softmax function for an output $p_i$ involves the term $p_i(1 - p_i)$. \n",
    "* If $p_i$ is near **1** (the winner), $1 - p_i$ is near **0**.\n",
    "* If $p_i$ is near **0** (the losers), $p_i$ is **0**.\n",
    "\n",
    "In both cases, the gradient becomes nearly zero. If the gradients vanish, the model's weights cannot be updated during backpropagation, effectively \"killing\" the learning process.\n",
    "\n",
    "#### 3. The Solution: Scaling (Figure B)\n",
    "By dividing the scores by $\\sqrt{d_k}$, we control the variance and pull it back toward **1**. As seen in **Figure B**, the attention weights are now much more \"distributed\" and \"smooth.\" This allows the model to:\n",
    "1. **Aggregate information** from multiple relevant tokens simultaneously.\n",
    "2. **Maintain healthy gradients**, ensuring that the model continues to learn and refine its understanding of relationships throughout the training process.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621ebbd",
   "metadata": {},
   "source": [
    "## Experiment 4: The Padding Mask Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e723c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padded tokens: \"<PAD>\", \"<PAD>\" (to reach max_length of 5)\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"<PAD>\", \"<PAD>\"]\n",
    "seq_len = len(tokens)\n",
    "\n",
    "# 2. Create the Padding Mask\n",
    "# 1 indicates a 'real' token, 0 indicates a 'padding' token\n",
    "# Mask shape: (1, 1, 5) to allow broadcasting across Query and Key dimensions\n",
    "mask = torch.tensor([[[1, 1, 1, 0, 0]]]) \n",
    "\n",
    "# 3. Simulate Attention Scores (Pre-softmax)\n",
    "torch.manual_seed(42)\n",
    "raw_scores = torch.randn(1, seq_len, seq_len)\n",
    "\n",
    "# --- Experiment A: Without Masking ---\n",
    "# The model treats <PAD> tokens as valid semantic information,not 0.00\n",
    "weights_no_mask = F.softmax(raw_scores, dim=-1)\n",
    "\n",
    "# --- Experiment B: With Padding Mask ---\n",
    "# This ensures that e^(-1e9) becomes 0 during the Softmax operation\n",
    "masked_scores = raw_scores.masked_fill(mask == 0, -1e9)\n",
    "weights_with_mask = F.softmax(masked_scores, dim=-1)\n",
    "\n",
    "# 4. Visualization for Portfolio Presentation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot A: No Masking\n",
    "sns.heatmap(weights_no_mask[0].detach().numpy(), annot=True, fmt=\".2f\", cmap='mako',\n",
    "            xticklabels=tokens, yticklabels=tokens, ax=ax1)\n",
    "ax1.set_title(\"Figure A: Standard Attention (No Mask)\\nNotice non-zero attention on <PAD>\")\n",
    "\n",
    "# Plot B: With Padding Mask\n",
    "sns.heatmap(weights_with_mask[0].detach().numpy(), annot=True, fmt=\".2f\", cmap='mako',\n",
    "            xticklabels=tokens, yticklabels=tokens, ax=ax2)\n",
    "ax2.set_title(\"Figure B: Masked Attention\\nAttention on <PAD> is forced to 0.00\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/padding_mask_effect_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bcadb",
   "metadata": {},
   "source": [
    "### Critical Analysis: Padding Masks\n",
    "\n",
    "In practical NLP pipelines, sentences in a batch often have different lengths. To enable parallel processing, shorter sentences are \"padded\" to match the **longest one**. However, these padding tokens`<PAD>` do not contain semantic value.\n",
    "\n",
    "**Observations:**\n",
    "* **Without Masking (Fig A):** The model incorrectly distributes \"attention energy\" to the `<PAD>` tokens. This means the representation of the word **\"cat\"** is being influenced by empty placeholders, leading to sub-optimal embeddings.\n",
    "* **With Masking (Fig B):** By applying a `-1e9` constant to the padding positions before the Softmax, we mathematically ensure their probability becomes zero ($e^{-1e9} \\approx 0$). \n",
    "\n",
    "**Conclusion:**\n",
    "It allows the Transformer to process batches of varying lengths simultaneously while ensuring the self-attention mechanism strictly focuses on **relevant linguistic context**, effectively \"blinding\" the model to the **synthetic noise** of padding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf61cf",
   "metadata": {},
   "source": [
    "## Experiment 5: Multi-Head Attention vs. Computational Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads # Each head gets a subspace of 64 dimensions\n",
    "\n",
    "# A sequence of 10 words\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"Original Shape: {x.shape} (1 Batch, 10 Words, 512 Features)\")\n",
    "\n",
    "# Reshaping to (Batch, Heads, Seq_Len, d_k)\n",
    "# This allows the model to process 8 \"views\" in parallel\n",
    "x_multi_head = x.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "\n",
    "print(f\"Multi-Head Shape: {x_multi_head.shape} (1 Batch, 8 Heads, 10 Words, 64 Features)\")\n",
    "print(f\"\\nResult: The model now has {num_heads} independent 'detectives', each focusing on a {d_k}-dimensional subspace.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92b82c",
   "metadata": {},
   "source": [
    "### Critical Analysis: Multi-Head Attention vs. Computational Efficiency\n",
    "\n",
    "**Logistics 1. What happens with different  values? With multiple heads?**\n",
    "\n",
    "* **Different  values (Dimension Size):**\n",
    "    * **Small :** The dot products are small. The attention weights are often too uniform(flat). The model cannot distinguish well between important and unimportant words.\n",
    "    * **Large :** The dot products become huge. The Softmax function makes the weights \"peaky\" (one value is near 1.0, others are near 0.0). This causes **vanishing gradients**, meaning the model stops learning.\n",
    "    * *Solution:* This is why we divide by  (scaling). It keeps the numbers in a good range for training.\n",
    "\n",
    "\n",
    "* **Multiple Heads:**\n",
    "    * **Single Head:** The model only has one \"view\" of the sentence. It might focus only on syntax (grammar) OR semantics (meaning), but not both.\n",
    "    * **Multiple Heads:** The model splits the embedding into several smaller parts. Each head learns to focus on **different features**.\n",
    "    * *Example:* Head 1 focuses on \"who did it\" (Subject-Verb). Head 2 focuses on \"when it happened\" (Time). The model captures complex relationships better using heatmap overlap.\n",
    "\n",
    "\n",
    "**Logistics 2. Self-attention: What patterns emerge?**\n",
    "* **The Diagonal Pattern:** The strongest attention is usually on the **diagonal line** of the heatmap. This means every word pays the most attention to **itself**.\n",
    "* **Semantic Linking:** Words pay attention to other related words, even if they are far away.\n",
    "* *Example:* In \"The **bank** near the river gave me **money**,\" the word \"bank\" will have high attention weights for \"river\" and \"money\" to understand context.\n",
    "* **Repetition:** If a word appears twice, they will strongly attend to each other.\n",
    "\n",
    "**Analysis 1. The Power of Diversity**\n",
    "It splits the $d_{model}$ into $h$ subspaces. \n",
    "* **The Single-Head Limitation:** A single head is forced to average all relationships. It might focus on the **syntax** but miss the **nuance** (e.g., \"sarcasm\" or \"temporal context\").\n",
    "* **The Multi-Head Advantage:** With 8 or 16 heads, the model can multi-task. \n",
    "    * *Head 1* might track **Grammar** (Who did what?).\n",
    "    * *Head 2* might track **Time** (When did it happen?).\n",
    "    * *Head 3* might track **Entity relationships** .\n",
    "The final output is a concatenation of these diverse \"perspectives,\" creating a far more robust representation than any single head could achieve.\n",
    "\n",
    "**Analysis 2. The Heavy Price: $O(n^2)$ Complexity**\n",
    "However, this intelligence comes with a significant cost. The core of the attention mechanism is the $QK^T$ matrix multiplication, which has a **spatial and temporal complexity of $O(n^2)$**, where $n$ is the sequence length.\n",
    "\n",
    "* **The Scaling Issue:** As the number of heads and sequence length increase, the memory requirement grows quadratically. For a sequence of 512 tokens, the attention matrix is $512 \\times 512$. But for a long document of 16,000 tokens, the matrix explodes to $256,000,000$ elements per head!\n",
    "\n",
    "* **Trade-off: Is it always worth it?**\n",
    "    * **For Short Text (Chatbots):** More heads are almost always better as they capture richer context.\n",
    "    * **For Long-Form Analysis (Books/Legal Docs):** The $O(n^2)$ bottleneck makes standard Multi-Head Attention nearly impossible to run on consumer hardware. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852935b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
